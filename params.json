{"name":"Image near replica detection","tagline":"Distributed near replica detector using Spark.","body":"## Welcome\r\n\r\nThis is a near replica detection framework mainly inspired by the work presented by Dong et al in \"High-Confidence Near-Duplicate Image Detection\". This work was done for the final thesis of the integrated Master in Computer Science at Universitat Polit√®cnica de Catalunya.\r\n\r\nThe purpose of this framework is to detected replicas (such as memes, resized replicas, collages, etc) of images in a distributed, efficient and real-time way. It is based on the computational framework Apache Spark and the distributed key-value database HBase. It contains several use cases for both batch and streaming scenarios allowing to compute on both disk and memory.\r\n\r\nThough it is intented to be installed in a Spark cluster it can also be tested on a local Linux environment. First, download [Spark](https://spark.apache.org/downloads.html) and [HBase](https://www.apache.org/dist/hbase/0.98.12.1/). Versions used during the development were 1.2.0 for Spark and 0.98 for HBase. Spark does not need much configuration but HBase needs some parameters to be adjusted. Note that if you use an existing cluster you must replicate your configuration to all your nodes.\r\n\r\n[OpenCV 2.4.9](http://opencv.org/downloads.html) has been the tool chosen for the image processing tasks and should also be installed on all nodes in the cluster and be accessible.\r\n\r\n## How it works\r\n\r\nThis section tries to give a glance of how this implementation indexed images and retrieves potential replicas of input images. First of all, we detect the interest points on the images and build descriptors around it (feature vectors). To restrict the number of features per image we resize the image maintaining its ratio (the bigger the image the more features are detected) and we also can apply a filtering on their entropy or variance to discard the weakest ones. Also, for some descriptors, it is recommended to log scale their values to make their distribution more uniform.\r\n\r\nEach feature is sketched using a hashing function so each value from the descriptor vector is mapped into one bit. By doing this we reduce the dimensionality of the features and we embed them into a Hamming space. \r\n\r\nThe binary sketch is divided into 'm' same size blocks and each one is put into a hash table using the block as a key. Two images match if they have at least one matching feature. Two features match when their Hamming distance is above a threshold 'h'. Candidate features that approximate the Hamming distance are those that match at least 'm' - 'h' blocks. \r\n\r\nFrom the list of candidates we discard those below the hamming threshold and we group the list of feature matches found by the image they belong to and assign a weight on them depending on the number of features that matched with the query image. We finally can apply another threshold to discard those results below a certain number of matches.\r\n\r\n### Requirements\r\n\r\n- OpenCV 2.4.9\r\n- Spark 1.2.0 or above\r\n- HBase 0.98\r\n- Maven 3.0.4 or above\r\n\r\n## Set up\r\n\r\nDownload Spark and HBase. Once you have downloaded HBase you must modify the following files in the conf/hbase-site.xml:\r\n\r\n```\r\n\t<configuration>\r\n\t  <property>\r\n\t    <name>hbase.rootdir</name>\r\n\t    <value>file:///path/to/data</value>\r\n\t  </property>\r\n\t  <property>\r\n\t    <name>hbase.zookeeper.property.dataDir</name>\r\n\t    <value>/path/to/zookeeper</value>\r\n\t  </property>\r\n\t</configuration>\r\n```\r\n\r\nThe first property \"base.rootdir\" is the value of the path where hbase will install the data and \"hbase.zookeeper.property.dataDir\" is the folder related to the cluster manager. You do not need to create them,they will be created the first time you start the service.\r\n\r\nThis settings are thought for a local distribution of HBase, where each process needed is spawned in the java JVM. Other approaches are possible: pseudo-distributed (where each component in the cluster uses an own daemon) and totally distributed. If you need to configure any of them please check HBase documentation.\r\n\r\nTo disable annoying warnings and to ensure that Snappy compression works on HBase, you must point to both Hadoop and Snappy libraries. To do so, we recommend to copy or create a symbolic link to them in a folder (we used the hadoop/lib/native/ from our Hadoop folder) and make sure that 'libsnappy.so.1' and 'libhadoop.so.1.0.0' are acccessible from there. Set the path to these libraries as environment variables in your bashrc and also check that variable JAVA_HOME is set:\r\n\r\n````\r\nexport JAVA_HOME=/path/to/jvm/java7\r\nexport JAVA_LIBRARY_PATH=$JAVA_LIBRARY_PATH:/path/to/hadoop/lib/native\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/hadoop/lib/native\r\n````\r\n\r\nThen reload the shell by:\r\n\r\n````\r\nsource ~/.bashrc\r\n````\r\n\r\nMore information about this step can be found [here] (http://hbase.apache.org/book.html#compression).\r\n\r\nIn case your are deploying in a cluster (even two separate clusters, one for HBase and one for Spark) you must repeat these operations in each of your nodes.\r\n\r\nIn order to start HBase open a terminal, go to the installation folder and type:\r\n\r\n```\r\n$ ./bin/start-hbase.sh\r\n```\r\n\r\n## Data\r\n\r\nFor parameter tunning and testing we built a small dataset called UPCReplica dataset. It contains labeled data from replicas and background images using different simple image transformations such as rotation, Gaussian noise addition or occlusion. It can be found [here] (https://github.com/DaniUPC/UPCTwitter-social-data-set).\r\n\r\nWe will use this dataset for some of the examples described here.\r\n\r\n## Compile and packaging\r\n\r\nWe are going to compile and create the jar with maven. Check that it is installed. If not, install it through apt-get on Ubuntu:\r\n\r\n```\r\n$ sudo apt-get install maven\r\n```\r\n\r\nDownload the code and go to the main folder. Execute:\r\n\r\n```\r\n$ mvn compile\r\n$ mvn package\r\n```\r\n\r\nIt may require a bit and then you'll see a jar-with-dependencies inside the target folder. This is the jar that will be submitted into Spark.\r\n\r\nTo make it work properly we must install OpenCV in the local maven repository or it will not compile. To do it, type the following replacing OPENCV_PATH by the route to your OpenCV folder installation.\r\n\r\n```\r\n$ mvn install:install-file -DgroupId=opencv -DartifactId=opencv -Dpackaging=jar -Dversion=2.4.9 -Dfile=/usr/local/opencv-2.4.9/opencv-249.jar\r\n```\r\n\r\nThe instructions about how to install OpenCV in your machine are in [this link] (http://docs.opencv.org/doc/tutorials/introduction/linux_install/linux_install.html).\r\n\r\n\r\n## Executing jobs (read before starting)\r\n\r\nSome scripts have already been written to make easier to launch jobs. They contain the list of parameters needed to execute the task. Some of them are default parameters but a few must be defined for each specific execution environment (they have a header with the indication). The list of scripts can be found inside the folder 'script'. Though their names are descriptive enough they contain a short description as well as the description of their parameters.\r\n\r\nThe parameters you should always set are:\r\n\r\n- 'OPENCV_PATH'. Points to the library folder of your OpenCV installation folder and must contain the *.so files.\r\n- 'SPARK_BIN'. Points to the 'bin' folder of the Spark installation directory.\r\n- 'SPARK_MASTER'. Points to the spark master node. If you are using a single machine set to 'local[n]' where n is , at most, the number of cores in your machine.\r\n\r\nIf you want to use the disk version, set the following parameters in each of the tasks:\r\n\r\n- 'HBASE_MASTER': Path to the hbase master. Only needed for 'disk' persistence. On local set to '127.0.0.1'.\r\n- 'ZOOKEEPER_PORT': Zookeeper port. Only needed for 'disk' persistence. By default it must be 2181-\r\n- 'ZOOKEEPER_HOST': Zookeeper manager host. Only needed for 'disk' persistence. Set to '127.0.0.1' if executin on your local machine.\r\n- 'PERSISTENCE' must be set to 1 to enable HBase.\r\n\r\nOtherwise, if memory mode chosen, you must set the following parameter in all memory-based tasks:\r\n\r\n- 'MEM_FILE' points to the configuration file that stores the detector parameters.\r\n- 'PERSISTENCE' must be set to 1 to enable memory computation.\r\n\r\n## Batch processing\r\n\r\n### Batch use cases using HBase\r\n\r\nThis is the most mature version of the system and uses HBase as database to persist the indexed data and query against them. Let's start by indexing some images from a folder. Open a terminal, go to the main folder of the project and then execute:\r\n\r\n```\r\n$ ./scripts/initialize.sh\r\n```\r\n\r\nThis command will flush all data and store the parameters of the replica detector in the database. These parameters can be modified in the script and values such as 'number of tables' or 'descriptor type' can be modified. Make sure you have set the parameters for the HBase configuration explained in the previous section.\r\n\r\nThen let's index some images from a folder. Execute:\r\n\r\n```\r\n$ ./scripts/index_folder.sh\r\n```\r\n\r\nAnd remember to set the parameter 'FOLDER' pointing to the folder with the images. Let's query a picture by executing:\r\n\r\n```\r\n% ./scripts/query_img.sh\r\n```\r\n\r\nApart from all parameters related to HBase and Spark remember to provide the path to the image to query. It can be a local file or an URL. Spark console will output the replica results for the query (if any).\r\n\r\n### Batch use cases using memory\r\n\r\nMemory-based implementation still does not have any mechanism to load data from external sources (e.g. HBase or Hadop File System) and indexed data on memory is flushed after the task ends. We prepared some tasks that embed both index and query in the same application. \r\n\r\nLet's choose two folders so one contains images to index (parameter 'FOLDER') and the other one images to query (parameter 'QUERY_FOLDER'). Execute:\r\n\r\n```\r\n$ ./scripts/index_query_folder.sh\r\n```\r\nSpark will output the results for the images in the console.\r\n\r\n## Twitter Streaming use cases\r\n\r\nFor this scenarios we need new parameters to be set in the scripts. We need to point to the twitter4j libraries:\r\n\r\n````\r\nTWITTER4J_CORE=lib/twitter4j-core-3.0.3.jar\r\nTWITTER4J_STREAMING=lib/twitter4j-stream-3.0.3.jar\r\n````\r\n\r\nThis is already set and MUST NOT be modified. API Twitter keys must be provided in order to receive streams of tweets. You must register an account as Twitter Developer and create an application [here] (https://apps.twitter.com/). When your keys are ready, you must override the following parameters in the scripts:\r\n\r\n````\r\nCONSUMER_KEY=\"insert_consumer_key\" # Twitter consumer key \r\nCONSUMER_SECRET=\"insert_consumer_secret\" # Twitter consumer secret\r\nACCESS_TOKEN=\"insert_access_token\" # Twitter access token\r\nACCESS_TOKEN_SECRET=\"insert_access_token_secret\" # Twitter access token \r\n````\r\n\r\nStreaming keeps the connection open and it is only stopped when an error occurs or when it is manually stopped. Before that, it constantly outputs data about the streams received.\r\n\r\n### Streaming images using HBase\r\n\r\nFor indexing images using Twitter Streaming API we must execute the following command, ensuring that we set the proper parameters that we have already commented throughout this tutorial:\r\n\r\n````\r\n$ ./scripts/index_streaming.sh\r\n````\r\n\r\nThis command will start indexing images on those tweets containing picture resources. We can also query them by executing the script:\r\n\r\n````\r\n$ ./scripts/query_streaming.sh\r\n````\r\n\r\nRemember that HBase mode must have the 'PERSISTENCE' parameter to '1'.\r\n\r\n### Streaming images on memory\r\n\r\nTo test streaming on memory you must open a terminal and execute the following on the root folder of the project:\r\n\r\n````\r\n$ ./scripts/index_query.sh\r\n````\r\n\r\nThis application submitted receives tweets, parses the images and queries them against the ones indexed. Aftwerads, they are stored so future tweets can queried against them. Remember that memory mode must have the 'PERSISTENCE' parameter to '0'.\r\n\r\n### Evaluating the UPCReplica dataset\r\n\r\nIf you want to see the impact of different configurations on the UPCReplica dataset you can use the 'evaluation.sh' task.\r\n\r\nImages from the UPCReplica dataset can be pointed using text files. In this example we will use the files uploaded in the repository, that contain:\r\n\r\n- base.csv: Contains 13 images to index. Can be downloaded [here] (https://github.com/DaniUPC/near-image-replica-detection/blob/master/files/base.csv)\r\n- query.csv: Contains 47 images: 32 replicas from one of the images to index and 15 background images. Download [here] (https://github.com/DaniUPC/near-image-replica-detection/blob/master/files/query.csv).\r\n\r\nDownload the test files and update the paths in the 'evaluation.sh' script:\r\n\r\n- 'DATASET_SRC_FILE': path where 'base.csv' is stored.\r\n- 'DATASET_QUERY_FILE': path where 'query.csv' is stored.\r\n\r\nTwo more parameter need to be configured:\r\n\r\n- 'DATASET_PATH': Downloaded the [UPCReplica dataset] (https://github.com/DaniUPC/UPCTwitter-social-data-set) and write here the path to the 'dataset' subfolder.\r\n- 'DEST_FILE': Destination where to write the results.\r\n\r\nNow you can execute:\r\n\r\n````\r\n$ ./scripts/evaluation.sh\r\n````\r\n\r\nAnd collect your results.\r\n\r\n### Comments on performance\r\n\r\nExecuting on local machine starts to be slow after several thousands of indexed images. This is even more noticeable on the memory based implementation. Note that local execution must only be used for development and that the higher the number of tables in a experiment the longer it will take to both query and index.\r\n\r\n### Spark output\r\n\r\nSpark output can be configured by creating a 'log4j.properties' in the 'conf' Spark directory. More information about log4j can be found [here] (http://www.mkyong.com/logging/log4j-log4j-properties-examples/).\r\n\r\n### Using the code\r\n\r\nIf you want to use and extend the code from the detector (we will focus on Eclipse) note that you must create a user library for OpenCV as detailed [in this tutorial] (http://docs.opencv.org/doc/tutorials/introduction/desktop_java/java_dev_intro.html).\r\n\r\nYou will need 'Maven' and 'Git' plugins in Eclipse to import the project.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}